{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes the process to load the dataset used to train and test the model. The dataset I am using on this project is just as stupid as the network. The idea is just to learn more about recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../dataset/data.csv\"\n",
    "data = pd.read_csv(dataset_path, header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is not large enough to justify using the PyTorch `Dataset` utility class. However, I will use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StupidBotDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path, header=0)\n",
    "        self.questions = self.data[\"question\"]\n",
    "        self.answers = self.data[\"answer\"]\n",
    "        self.data_len = len(self.data.index)\n",
    "        \n",
    "        # Unique characters in the database.\n",
    "        self.unique_characters = set(\"\".join(self.questions + self.answers))\n",
    "        self.unique_characters_length = len(self.unique_characters)\n",
    "        # Map int to character.\n",
    "        self.int2char = dict(enumerate(self.unique_characters))\n",
    "        # Map character to int.\n",
    "        self.char2int = {char: i for i, char in self.int2char.items()}\n",
    "        \n",
    "        # Longer question.\n",
    "        longer_question_length = len(max(self.questions, key=len))\n",
    "        # Longer answer.\n",
    "        longer_answer_length = len(max(self.answers, key=len))\n",
    "        \n",
    "        # Pad strings.\n",
    "        self.questions = self.questions.str.pad(longer_question_length, side=\"right\")\n",
    "        self.answers = self.answers.str.pad(longer_answer_length, side=\"right\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.questions[index]\n",
    "        # Map text to int.\n",
    "        x = self.text2int(x)\n",
    "        # One-hot encode x.\n",
    "        x = self.one_hot_encode(x)\n",
    "        x = torch.tensor(x)\n",
    "        \n",
    "        y = self.answers[index]\n",
    "        # Map text to int.\n",
    "        y = self.text2int(y)\n",
    "        # One-hot encode y.\n",
    "        y = self.one_hot_encode(y)\n",
    "        y = torch.tensor(y)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def text2int(self, text):\n",
    "        \"\"\"\n",
    "            Convert text to an array of integers.\n",
    "        \"\"\"\n",
    "        return [self.char2int[c] for c in text]\n",
    "    \n",
    "    def one_hot_encode(self, sequence):\n",
    "        \"\"\"\n",
    "            Convert an array of integers to a matrix one-hot encoded.\n",
    "        \"\"\"\n",
    "        encoded = np.zeros([self.unique_characters_length, len(sequence)], dtype=int)\n",
    "        for i, character in enumerate(sequence):\n",
    "            encoded[character][i] = 1\n",
    "        return encoded\n",
    "    \n",
    "    def one_hot_decode(self, sequence):\n",
    "        \"\"\"\n",
    "            sequence: PyTorch tensor.\n",
    "        \"\"\"\n",
    "        return [np.argmax(x) for x in sequence.numpy().T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows an example of how to use the `StupidBotDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StupidBotDataset(dataset_path)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to divide the dataset into training and testing. To do this, I will use the tools provided by  PyTorch.\n",
    "\n",
    "The dataset will be loaded and shuffled. In large datasets, this can be a problem. However, as this dataset is small, I will use this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and define the parameters used to split and load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StupidBotDataset(dataset_path)\n",
    "dataset_size = len(dataset)\n",
    "dataset_indices = list(range(dataset_size))\n",
    "\n",
    "batch_size = 1\n",
    "test_split = int(np.floor(0.2 * dataset_size))  # 20%\n",
    "# Shuffle dataset indices.\n",
    "np.random.shuffle(dataset_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = (\n",
    "    dataset_indices[test_split:],\n",
    "    dataset_indices[:test_split],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
